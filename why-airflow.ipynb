{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T16:26:44.982465Z",
     "start_time": "2018-11-30T16:26:43.433365Z"
    }
   },
   "source": [
    "Modern Big Data Tool (Apache Airflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow workflow management platform, support schedule and monitor workflows.\n",
    "\n",
    "Data workflow problem such as sequential task and schedule task, we have to deal with\n",
    "- Dependent task\n",
    "- Schedule job\n",
    "- Manage and monitor\n",
    "- Error handling e.g. send email when it fails\n",
    "\n",
    "<img src=\"./asset/dag-example.png\" title='https://medium.com/@dustinstansbury/understanding-apache-airflows-key-concepts-a96efed52b1a' style=\"width: 700px;\"/>\n",
    "\n",
    "There are a number of ways to do data workflows:\n",
    "(you can check the detailed comparison on the link in bottom of the report)\n",
    "- Custom scripts\n",
    "- Cron, based on timing and run-time of each task\n",
    "- Oozie, Luigi\n",
    "- **Airflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "- Monitoring\n",
    "- Scheduler, fire task that ready to run\n",
    "- Rich CLI (command line interface) e.g. to test, backfill, describe task\n",
    "- Easier test e.g. `airflow test <dagId> <taskId>`\n",
    "- Easy-to-use UI\n",
    "    - Explore your DAGs definition\n",
    "    - Task status, summary, dependencies, progress\n",
    "    - Metadata\n",
    "    - Logs\n",
    "    - Task stats e.g. tasks latencies over time\n",
    "    - Enable / Disable task via UI\n",
    "    - etc.\n",
    "- Task hierarchy / Dependency management\n",
    "- Task logs\n",
    "- Re-run historical tasks and task action such as \"mark\" and \"force\"\n",
    "- Historic pipeline runs / Backfill concept\n",
    "- Strong open source e.g. The gitter (chatroom) for airflow is very active\n",
    "- Many extensions\n",
    "- Many support many of big data ecosystem tools e.g. S3, Druid, HDFS, Hive, Sqoop, etc., that was a great benefit if you want to working on a big data ecosystem.\n",
    "- Scalable, design for distributed task queue\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases of Airflow at Airbnb:\n",
    "\n",
    "1. Data warehousing: ETL, cleanse, organize, data quality check and publish data into data warehouse\n",
    "2. Growth analytics: compute metrics around e.g. guest and host engagement\n",
    "3. Experimentation: compute our A/B testing experimentation\n",
    "4. Email targeting\n",
    "5. Sessionization: compute clickstream and time spent datasets\n",
    "6. Search: compute search ranking related metrics\n",
    "7. Data infrastructure maintenance: database scrapes, folder cleanup\n",
    "\n",
    "<img src=\"./asset/example-ui-graph.png\" style=\"width: 800px;\" title=\"https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8\"/>\n",
    "<img src=\"./asset/example-ui-tree.png\" style=\"width: 800px;\" title=\"https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & installation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual machine (VM)\n",
    "\n",
    "Actually, it's required only `Python` and `pip` installation. So, the system can be any as long as we can install `Python` and `pip` but from our previous exploration, we recommend to use linux based such as CentOS.\n",
    "\n",
    "In this project, we're going to use\n",
    "- CentOS 6\n",
    "- Python 2\n",
    "- MongoDB server, that's already been installed\n",
    "\n",
    "MongoDB server will being used for the example usage\n",
    "(it's not main focus on this project)\n",
    "\n",
    "### Create Firewall rules\n",
    "<img src=\"./asset/firewall-port.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "### Create VM instance\n",
    "\n",
    "Create new VM based on \"centos-basic-image\" image.\n",
    "\n",
    "In case you don't have \"centos-basic-image\" image, you basically just need to install `Python 2` and `pip`\n",
    "\n",
    "<img src=\"./asset/create-vm.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Install MongoDB (already been taught in class)\n",
    "\n",
    "### Access VM instance that we created\n",
    "\n",
    "Such as `ssh admin@airflow-project`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add environment variables\n",
    "\n",
    "Add these environment variables into `~/.bash_profile` file, `AIRFLOW_HOME` will store its configuration and our SQlite database (if we using default database).\n",
    "\n",
    "```\n",
    "export SLUGIFY_USES_TEXT_UNIDECODE=yes\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "```\n",
    "\n",
    "Then reload config by `source .bash_profile`\n",
    "\n",
    "![](./asset/env-var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install module\n",
    "\n",
    "```\n",
    "sudo yum install gcc\n",
    "sudo yum install gcc-c++\n",
    "sudo pip2 install --upgrade pip\n",
    "\n",
    "sudo SLUGIFY_USES_TEXT_UNIDECODE=yes pip2 install apache-airflow\n",
    "sudo pip2 install flask_bcrypt ccxt pymongo # for example code\n",
    "\n",
    "# it will create `/home/admin/airflow`\n",
    "# also initialize database for Airflow\n",
    "airflow initdb\n",
    "# to check current version of it\n",
    "airflow version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup authentication for Airflow web UI\n",
    "\n",
    "### Update authenticate setting\n",
    "\n",
    "```\n",
    "nano /home/admin/airflow/airflow.cfg\n",
    "find\n",
    "    authenticate = False\n",
    "then change to\n",
    "    authenticate = True\n",
    "    auth_backend = airflow.contrib.auth.backends.password_auth\n",
    "```\n",
    "\n",
    "### Create new user\n",
    "\n",
    "Default database is a `SQLite` database, which is fine for this project. In a production, you'll probably have to using something else like MySQL or PostgreSQL.\n",
    "\n",
    "These Python code below will create new user into SQLite db\n",
    "\n",
    "```\n",
    "python2\n",
    "\n",
    "import airflow\n",
    "from airflow import models, settings\n",
    "from airflow.contrib.auth.backends.password_auth import PasswordUser\n",
    "user = PasswordUser(models.User())\n",
    "user.username = 'admin2'\n",
    "user.email = 'admin@example.com'\n",
    "user.password = 'qwejqwhisrw948'\n",
    "session = settings.Session()\n",
    "session.add(user)\n",
    "session.commit()\n",
    "session.close()\n",
    "exit()\n",
    "```\n",
    "\n",
    "<img src=\"./asset/new-user.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload example files\n",
    "\n",
    "Upload `dags` folder into `/home/admin/airflow`, contains example DAG files that we prepare for demo purpose (you can setup and change its location in `airflow.cfg`).\n",
    "\n",
    "At the end you'll see structure like this\n",
    "```\n",
    "airflow                     # root directory.\n",
    "├─ dags                     # contains all DAG files\n",
    "| |...\n",
    "| ├── my_tutorial.py        # DAG definitions\n",
    "| ├── my_simple_dag.py\n",
    "| └── my_cryptocurrency.py\n",
    "├─ logs                     # logs for the various tasks that are run\n",
    "├─ airflow-webserver.pid\n",
    "├─ airflow.cfg              # global configuration for Airflow\n",
    "├─ airflow.db               # SQLite database used by Airflow internally to track status of each DAG.\n",
    "└─ unittests.cfg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start\n",
    "\n",
    "Open 2 terminals\n",
    "1. First terminal run scheduler: `$ airflow scheduler`\n",
    "2. Second terminal run web UI: `$ airflow webserver --port 8080`\n",
    "\n",
    "Now, you should be able to access Airflow web UI, then login with user we created\n",
    "- user: `admin2`\n",
    "- pass: `qwejqwhisrw948`\n",
    "\n",
    "![](./asset/login-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept and components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow scheduler\n",
    "\n",
    "Monitoring process that runs all the time, triggers task execution based on `schedule_interval` and `start_date`.\n",
    "\n",
    "For example\n",
    "\n",
    "```\n",
    "start_date: 2018-9-24\n",
    "schedule_interval: */1 * * * *\n",
    "```\n",
    "\n",
    "So, task will be executed at \n",
    "- 2018-9-24 00:01:00\n",
    "- 2018-9-24 00:02:00\n",
    "- 2018-9-24 00:03:00 and so on\n",
    "\n",
    "\n",
    "## DAG / Workflows\n",
    "\n",
    "An Airflow workflow is designed as a directed acyclic graph (DAG).\n",
    "\n",
    "For example in \"home-price-scrapper\" process you may want some thing like this\n",
    "\n",
    "```\n",
    "get_data_from_goodhome.com --> cleaning -┐\n",
    "get_data_from_home.com     --> cleaning  |-> merged -> save into db\n",
    "get_data_from_homedd.com   --> cleaning -┘\n",
    "```\n",
    "\n",
    "More examples\n",
    "\n",
    "![](./asset/dag.png)\n",
    "![](./asset/dag2.png)\n",
    "\n",
    "### Dependency\n",
    "\n",
    "Dependency's defined by code\n",
    "\n",
    "#### No dependency, it will be executed parallely.\n",
    "\n",
    "<img src=\"./asset/dependency-none.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "#### Sequential\n",
    "\n",
    "Such as `opr_hello >> opr_greet >> opr_sleep >> opr_respond`\n",
    "<img src=\"./asset/dependency1.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### Others\n",
    "\n",
    "Such as `opr_hello >> opr_greet >> opr_sleep << opr_respond`\n",
    "<img src=\"./asset/dependency2.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "## DagRun (DAG runs)\n",
    "\n",
    "A DagRun is the instance of a DAG. Airflow will generate DAG runs from the `start_date` with the specified `schedule_interval` that we define in DAG file.\n",
    "\n",
    "Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the `start_date`. Any missing DAG runs are automatically scheduled.\n",
    "\n",
    "e.g. DagRun for `my_simple_dag`, each task run parallelly\n",
    "\n",
    "```\n",
    "opr_hello = BashOperator(task_id='say_Hi', bash_command='echo \"Hi!!\"')\n",
    "opr_greet = PythonOperator(task_id='greet', python_callable=greet)\n",
    "opr_sleep = BashOperator(task_id='sleep_me', bash_command='sleep 5')\n",
    "opr_respond = PythonOperator(task_id='respond', python_callable=respond)\n",
    "\n",
    "+--------------------------------+\n",
    "|             DagRun             |\n",
    "|                                |\n",
    "| +------------+  +------------+ |\n",
    "| | opr_hello  |  | opr_greet  | |\n",
    "| +------------+  +------------+ |\n",
    "| +------------+  +------------+ |\n",
    "| | opr_sleep  |  | opr_respond| |\n",
    "| +------------+  +------------+ |\n",
    "+--------------------------------+\n",
    "```\n",
    "\n",
    "## Backfilling\n",
    "\n",
    "For example, we run web scrapper hourly to get data by using current date to get specific data such as `http://getdata/?date=<current-datetime>` then the script broken last week. What we may have to do is run it manually\n",
    "\n",
    "Airflow have ability to solve that issue by using `start_date` and `schedule_interval` including context API provided by Airflow For example, when you initialize on `2016-01-04` a DAG with a `start_date` at `2016-01-01`\n",
    "and a daily `schedule_interval`, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04.\n",
    "\n",
    "## Operator\n",
    "\n",
    "Operators allow for generation of certain types of tasks that become nodes in the DAG when instantiated.\n",
    "\n",
    "There are 3 main types of operators:\n",
    "- Operators that performs an action, or tell another system to perform an action e.g. `BashOperator`, `PythonOperator`, `SimpleHttpOperator`, `MySqlOperator`, etc\n",
    "- Transfer operators move data from one system to another e.g. `HiveToMySqlTransfer`, `MsSqlToHiveTransfer`, etc.\n",
    "- Sensors are a certain type of operator that will keep running until a certain criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "Metadata structure that was stored in Airflow database (e.g. `airflow.db`).\n",
    "\n",
    "<img src=\"./asset/airflowdb-dag.png\" style=\"width: 800px;\"/>\n",
    "<img src=\"./asset/airflowdb-dag-run.png\" style=\"width: 800px;\"/>\n",
    "<img src=\"./asset/airflowdb-dag-stats.png\" style=\"width: 800px;\"/>\n",
    "<img src=\"./asset/airflowdb-job.png\" style=\"width: 800px;\"/>\n",
    "<img src=\"./asset/airflowdb-log.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web UI\n",
    "\n",
    "## Example\n",
    "\n",
    "<img src=\"./asset/ui-dashboard-example.png\" style=\"width: 900px;\"/>\n",
    "<img src=\"./asset/ui-log-example.png\" style=\"width: 900px;\"/>\n",
    "<img src=\"./asset/ui-task-example.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
